{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1nDGR4ZOP5F",
        "outputId": "e8627646-d8c4-4531-95d3-19a44e8bad0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lexical analysis saved to /usr/colab/bin/Excel/Lexical_analysis_prompts.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "file_path = '/usr/colab/bin/Excel/Attribution_scores.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Lexical analysis on a token\n",
        "def lexical_analysis(token):\n",
        "    if not isinstance(token, str):\n",
        "        token = str(token)\n",
        "    doc = nlp(token)\n",
        "    analysis = {\n",
        "        \"Token\": [],\n",
        "        \"POS\": [],\n",
        "        \"Lemma\": [],\n",
        "        \"Entity\": [],\n",
        "        \"Entity Type\": []\n",
        "    }\n",
        "    for t in doc:\n",
        "        analysis[\"Token\"].append(t.text)\n",
        "        analysis[\"POS\"].append(t.pos_)\n",
        "        analysis[\"Lemma\"].append(t.lemma_)\n",
        "        if t.ent_type_:\n",
        "            analysis[\"Entity\"].append(t.text)\n",
        "            analysis[\"Entity Type\"].append(t.ent_type_)\n",
        "    return analysis\n",
        "\n",
        "# Apply the lexical analysis to each token in the DataFrame\n",
        "lexical_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    token = row['Prompt']\n",
        "    analysis = lexical_analysis(token)\n",
        "    for i in range(len(analysis[\"Token\"])):\n",
        "        lexical_data.append({\n",
        "            \"Model\": row[\"Model\"],\n",
        "            \"Response\": row[\"Response\"],\n",
        "            \"Prompt\": row[\"Prompt\"],\n",
        "            \"Attribution Score\": row[\"Score\"],\n",
        "            \"Token\": analysis[\"Token\"][i],\n",
        "            \"POS\": analysis[\"POS\"][i],\n",
        "            \"Lemma\": analysis[\"Lemma\"][i],\n",
        "            \"Entity\": analysis[\"Entity\"][i] if i < len(analysis[\"Entity\"]) else None,\n",
        "            \"Entity Type\": analysis[\"Entity Type\"][i] if i < len(analysis[\"Entity Type\"]) else None\n",
        "        })\n",
        "\n",
        "lexical_df = pd.DataFrame(lexical_data)\n",
        "\n",
        "output_file_path = '/usr/colab/bin/Excel/Lexical_analysis_prompts.xlsx'\n",
        "lexical_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f'Lexical analysis saved to {output_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "lexical_file_path = '/usr/colab/bin/Excel/Lexical_analysis_prompts.xlsx'\n",
        "lexical_df = pd.read_excel(lexical_file_path)\n",
        "\n",
        "print(lexical_df.describe())\n",
        "\n",
        "# Count the occurrences of each POS tag\n",
        "pos_counts = lexical_df['POS'].value_counts()\n",
        "print(pos_counts)\n",
        "\n",
        "# Count the occurrences of each entity type\n",
        "entity_type_counts = lexical_df['Entity Type'].value_counts()\n",
        "print(entity_type_counts)\n",
        "\n",
        "# Average attribution score by POS tag\n",
        "avg_score_by_pos = lexical_df.groupby('POS')['Attribution Score'].mean()\n",
        "print(avg_score_by_pos)\n",
        "\n",
        "# Average attribution score by entity type\n",
        "avg_score_by_entity = lexical_df.groupby('Entity Type')['Attribution Score'].mean()\n",
        "print(avg_score_by_entity)\n",
        "\n",
        "# Count of tokens per model\n",
        "tokens_per_model = lexical_df.groupby('Model').size()\n",
        "print(tokens_per_model)\n",
        "\n",
        "# Average attribution score per model\n",
        "avg_score_per_model = lexical_df.groupby('Model')['Attribution Score'].mean()\n",
        "print(avg_score_per_model)\n",
        "\n",
        "# POS tag distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=lexical_df, x='POS', order=lexical_df['POS'].value_counts().index)\n",
        "plt.title('POS Tag Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('/usr/colab/bin/Excel/pos_tag_distribution_promts.png')\n",
        "plt.close()\n",
        "\n",
        "# Average attribution score by POS tag\n",
        "plt.figure(figsize=(10, 6))\n",
        "avg_score_by_pos.plot(kind='bar')\n",
        "plt.title('Average Attribution Score by POS Tag')\n",
        "plt.xlabel('POS Tag')\n",
        "plt.ylabel('Average Attribution Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('/usr/colab/bin/Excel/avg_score_by_pos_promts.png')\n",
        "plt.close()\n",
        "\n",
        "# Average attribution score by entity type\n",
        "plt.figure(figsize=(10, 6))\n",
        "avg_score_by_entity.plot(kind='bar')\n",
        "plt.title('Average Attribution Score by Entity Type')\n",
        "plt.xlabel('Entity Type')\n",
        "plt.ylabel('Average Attribution Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('/usr/colab/bin/Excel/avg_score_by_entity_promts.png')\n",
        "plt.close()\n",
        "\n",
        "# Average attribution score per model\n",
        "plt.figure(figsize=(10, 6))\n",
        "avg_score_per_model.plot(kind='bar')\n",
        "plt.title('Average Attribution Score per Model')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Average Attribution Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('/usr/colab/bin/Excel/avg_score_per_model_promts.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1_KsCETOSd5",
        "outputId": "01fec6cf-2b12-4b13-ad81-8053b979af2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Attribution Score\n",
            "count        2529.000000\n",
            "mean            3.084553\n",
            "std             5.545881\n",
            "min           -11.198800\n",
            "25%             1.054100\n",
            "50%             1.844200\n",
            "75%             3.781000\n",
            "max            39.013900\n",
            "POS\n",
            "PUNCT    546\n",
            "VERB     427\n",
            "NOUN     393\n",
            "PRON     309\n",
            "PROPN    211\n",
            "AUX      103\n",
            "ADJ       90\n",
            "ADV       89\n",
            "ADP       79\n",
            "SPACE     69\n",
            "PART      52\n",
            "NUM       44\n",
            "CCONJ     42\n",
            "X         24\n",
            "INTJ      23\n",
            "SCONJ     19\n",
            "SYM        5\n",
            "DET        4\n",
            "Name: count, dtype: int64\n",
            "Entity Type\n",
            "ORG         48\n",
            "PERSON      42\n",
            "GPE         32\n",
            "CARDINAL    21\n",
            "ORDINAL     20\n",
            "DATE        16\n",
            "LANGUAGE     8\n",
            "NORP         6\n",
            "LAW          2\n",
            "Name: count, dtype: int64\n",
            "POS\n",
            "ADJ      3.954572\n",
            "ADP      1.459161\n",
            "ADV      1.893254\n",
            "AUX      2.688559\n",
            "CCONJ    3.064188\n",
            "DET      1.637975\n",
            "INTJ     3.034622\n",
            "NOUN     3.760212\n",
            "NUM      3.630470\n",
            "PART     2.846104\n",
            "PRON     1.772848\n",
            "PROPN    3.796934\n",
            "PUNCT    3.276727\n",
            "SCONJ    2.991900\n",
            "SPACE    3.873259\n",
            "SYM      0.718080\n",
            "VERB     3.270722\n",
            "X        1.305717\n",
            "Name: Attribution Score, dtype: float64\n",
            "Entity Type\n",
            "CARDINAL    2.806900\n",
            "DATE        4.946381\n",
            "GPE         4.031328\n",
            "LANGUAGE    2.790987\n",
            "LAW         8.761850\n",
            "NORP        3.807233\n",
            "ORDINAL     0.908320\n",
            "ORG         5.037915\n",
            "PERSON      3.789657\n",
            "Name: Attribution Score, dtype: float64\n",
            "Model\n",
            "Gemma       855\n",
            "Llama       378\n",
            "Mistral    1296\n",
            "dtype: int64\n",
            "Model\n",
            "Gemma      4.434063\n",
            "Llama      2.180351\n",
            "Mistral    2.457977\n",
            "Name: Attribution Score, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/usr/colab/bin/Excel/Lexical_analysis_prompts.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Top and bottom 25 attribution scores for each model\n",
        "def get_top_bottom_attributions(df):\n",
        "    models = df['Model'].unique()\n",
        "    result = pd.DataFrame()\n",
        "\n",
        "    for model in models:\n",
        "        model_df = df[df['Model'] == model]\n",
        "        top_25 = model_df.nlargest(25, 'Attribution Score')\n",
        "        bottom_25 = model_df.nsmallest(25, 'Attribution Score')\n",
        "        result = pd.concat([result, top_25, bottom_25], ignore_index=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "top_bottom_attributions = get_top_bottom_attributions(df)\n",
        "\n",
        "output_file_path = '/usr/colab/bin/Excel/top_bottom_attributions_promts.xlsx'\n",
        "top_bottom_attributions.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(\"The top and bottom 25 attribution scores for each model have been saved to:\", output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfKN0xzcOeK9",
        "outputId": "881b93e9-2ae6-453e-ea49-007d1dea7557"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top and bottom 25 attribution scores for each model have been saved to: /usr/colab/bin/Excel/top_bottom_attributions_promts.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Lemmas and Models, calculate occurrences and summary statistics\n",
        "grouped_data = lexical_df.groupby(['Model', 'Lemma']).agg({\n",
        "    'Attribution Score': ['count', 'min', 'max', 'mean']\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns\n",
        "grouped_data.columns = ['Model', 'Lemma', 'Occurrences', 'Min Score', 'Max Score', 'Average Score']\n",
        "\n",
        "# Find the top 50 occurring lemmas per model\n",
        "top_lemmas_per_model = grouped_data.groupby('Model').apply(lambda x: x.nlargest(50, 'Occurrences')).reset_index(drop=True)\n",
        "\n",
        "output_file_path = '/usr/colab/bin/Excel/Top_lemmas_analysis.xlsx'\n",
        "top_lemmas_per_model.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f'Top lemmas analysis saved to {output_file_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJu5prqeOgba",
        "outputId": "891edefd-a23e-4918-d622-a26c56ca732b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top lemmas analysis saved to /usr/colab/bin/Excel/Top_lemmas_analysis.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/usr/colab/bin/Excel/Top_lemmas_analysis.xlsx'\n",
        "df_top_lemmas = pd.read_excel(file_path)\n",
        "\n",
        "print(\"Top 50 Lemmas per Model:\")\n",
        "print(df_top_lemmas)\n",
        "\n",
        "# Average attribution scores across all models grouped by Lemma\n",
        "lemma_avg_scores = df_top_lemmas.groupby('Lemma')['Average Score'].mean().reset_index()\n",
        "\n",
        "# Sort by average score\n",
        "top_high_impact_lemmas = lemma_avg_scores.sort_values(by='Average Score', ascending=False).head(25)\n",
        "\n",
        "print(\"\\nTop 25 Lemmas with Highest Average Attribution Scores:\")\n",
        "print(top_high_impact_lemmas[['Lemma', 'Average Score']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBvncY3tOjHK",
        "outputId": "8d54c0e0-4755-4f5b-efd0-67a38624c88a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 50 Lemmas per Model:\n",
            "       Model     Lemma  Occurrences  Min Score  Max Score  Average Score\n",
            "0      Gemma         \"           87    -5.2387    29.7843       4.881663\n",
            "1      Gemma                     69    -6.5543    30.7499       3.873259\n",
            "2      Gemma        be           49    -8.1535    12.9251       3.368822\n",
            "3      Gemma         ,           40    -3.1074    22.9291       4.751890\n",
            "4      Gemma         I           36    -1.8260     8.3815       2.489083\n",
            "..       ...       ...          ...        ...        ...            ...\n",
            "145  Mistral      hell           10    -1.3951    13.1247       3.538010\n",
            "146  Mistral  naysayer           10    -1.3890     4.9627       0.773670\n",
            "147  Mistral      Anti            9    -1.3723    16.1613       3.796822\n",
            "148  Mistral     Trump            9    -1.3723    16.1613       3.796822\n",
            "149  Mistral        dd            9   -11.1988    12.8812       0.585367\n",
            "\n",
            "[150 rows x 6 columns]\n",
            "\n",
            "Top 25 Lemmas with Highest Average Attribution Scores:\n",
            "            Lemma  Average Score\n",
            "41           beat      10.538757\n",
            "18         Carter       8.833520\n",
            "66           fuck       8.615756\n",
            "79          jigga       7.993775\n",
            "99         retort       7.970140\n",
            "33            all       7.903886\n",
            "60           f---       7.218483\n",
            "123         woman       7.160033\n",
            "105         smack       7.115960\n",
            "106         smash       7.035414\n",
            "24          Naith       6.587033\n",
            "27         Queens       6.053625\n",
            "95        proceed       5.992267\n",
            "67         fucker       5.944086\n",
            "25          Obama       5.933790\n",
            "77       infighte       5.815267\n",
            "39          b*tch       5.811763\n",
            "80           leak       5.780233\n",
            "61         faggot       5.779873\n",
            "85   motherfucker       5.709429\n",
            "48          chant       5.284714\n",
            "96           punk       5.201686\n",
            "23         Morgan       5.005414\n",
            "13           2017       4.916240\n",
            "121         whore       4.854814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZN222znbPL48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}